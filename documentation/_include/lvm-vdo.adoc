:sectnums:
:sectnumlevels: 3
:markup-in-source: verbatim,attributes,quotes
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:disk0: /dev/sda
:disk1: /dev/sdb
:disk2: /dev/sdc
:disk3: /dev/sdd
:disk4: /dev/sde
:disk_glob: /dev/sd{b..e}
:format_cmd_exec: source,options="nowrap",subs="{markup-in-source}",role="copy"
:format_cmd_output: bash,options="nowrap",subs="{markup-in-source}"
ifeval::["%cloud_provider%" == "ec2"]
:disk0: /dev/nvme0n1
:disk1: /dev/nvme1n1
:disk2: /dev/nvme2n1
:disk3: /dev/nvme3n1
:disk4: /dev/nvme4n1
:disk_glob: /dev/nvme{1..4}n1
:format_cmd_exec: source,options="nowrap",subs="{markup-in-source}",role="execute"
endif::[]



:toc:
:toclevels: 1

= LVM and VDO : Storage Management and Data Optimization

== Overview

A Logical Volume Manager is a software layer on top of physical hard disks and partitions, which creates an abstraction of continuity and ease-of-use for managing the lifecycle of those devices (ie: addition, removal, replacement, repartitioning, backup, etc).

Over the years, the role of LVM has expanded greatly to include data redundancy (RAID), compression, deduplication, and more...

These exercises will help you get familiar with the basic concepts of LVM and also introduce deduplication with the Virtual Data Optimizer (VDO).

== Getting Started

For these exercises, you will be using the host `node3` as user `root`.

From host `bastion`, ssh to `node3`.

[{format_cmd_exec}]
----
ssh node3
----

Use `sudo` to elevate your privileges.

[{format_cmd_exec}]
----
[[ "$UID" == 0 ]] || sudo -i
----

Verify that you are on the right host for these exercises.

[{format_cmd_exec}]
----
workshop-vdo-checkhost.sh
----

You are now ready to proceed with these exercises.

== Installation & Configuration

Install the required packages - this will pull in several related dependencies.

[{format_cmd_exec}]
----
dnf install -y vdo
----

That's it!  All LVM components are a standard part of RHEL.  Only the vdo kmod and related utilities need to be installed.

== Why use Logical Volume Management?

* Flexibility
* Grow, shrink or relocate your data/filesystems
* Aggregate or subdivide devices as needed
* Performance
* Striping across multiple devices
* Caching via SSDs
* Fault Tolerance (redundancy & resiliency)
* RAID 0, 1, 5, 6, 10
* Snapshots: Historical Recovery
* Data Optimization: Compression and De-Duplication

=== Building Blocks of Storage Management

From the bottom up, here is a basic explanation of the layered technology stack that comprises modern storage.

|===
| File-systems    | Formatted LV's become filesystems
| Logical Volume  | A virtual storage device that may span multiple physical devices. Allocatable chunks (PEs) are assembled into “Logical Extents” that form the addressable space.
| Volume Group    | A collection of Physical Volumes that are divided into discrete allocatable chunks called “physical extents” (PEs).
| Physical Volume | An LVM concept that identifies physical devices for LVM use.
| Physical Device | Disks (IDE [hda], SCSI, SATA & SAS [sda], etc...)
                    Partitions (ex: hda1, sda1, cciss/c0d0p1, etc...)
                    LUNs (FCOE, SAN, etc...)
                    loopback
|===

=== LVM CLI Toolbox

[options="header"]
|===
|                | Physical Volumes | Volumes Groups | Logical Volumes
| Core Utilities l| 
pvcreate
pvdisplay 
pvremove 
pvs 
pvscan 
pvmove
                 l| 
vgcreate 
vgdisplay
vgextend 
vgreduce 
vgremove 
vgrename 
vgs
vgscan
vgcfgbackup 
vgcfgrestore 
                 l| 
lvconvert
lvcreate
lvdisplay 
lvextend 
lvreduce 
lvremove 
lvrename 
lvresize 
lvs
lvscan

| Other Stuff    l| 
fdisk 
parted 
partprobe 
multipath 
smartd
                 |
                 l| 
mkfs
mount
fsadm
|===



== Create a Linear Volume

=== Summary

In this exercise, you will perform steps to make a new filesystem available to the system using the Logical Volume Management tools.  

We will begin with a simple linear volume (concatenation).

=== Clean Up Devices

Since we will be reusing the same resources for many exercises, we will begin by wiping everything clean.  Don't worry if you get an error message.

[{format_cmd_exec}]
----
umount /mnt/lab*
----

[{format_cmd_exec}]
----
vgremove -ff vg_lab
----

[{format_cmd_exec}]
----
pvremove {disk_glob}
----

[{format_cmd_exec}]
----
wipefs -a {disk_glob}
----

[{format_cmd_exec}]
----
partprobe
----

=== Physical Volume Creation

[{format_cmd_exec}]
----
pvcreate {disk1}
----

=== Volume Group (Pool) Creation

[{format_cmd_exec}]
----
vgcreate vg_lab {disk1}
----

=== Logical Volume Creation

[{format_cmd_exec}]
----
lvcreate -y -n lab1 -l 95%FREE vg_lab
----

=== Make and Mount Filesystem


[{format_cmd_exec}]
----
mkfs -t ext4 /dev/vg_lab/lab1
----

[{format_cmd_exec}]
----
mkdir -p /mnt/lab1
----

[{format_cmd_exec}]
----
mount /dev/vg_lab/lab1 /mnt/lab1
----

NOTE: If this were going to be a persistent filesystem, you would also need to add an entry to `/etc/fstab`.

=== Examine Your Work

[{format_cmd_exec}]
----
lvs
----

[{format_cmd_output}]
----
  LV   VG     Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  lab1 vg_lab -wi-ao---- <4.75g
----

[{format_cmd_exec}]
----
lvs vg_lab/lab1
----

[{format_cmd_output}]
----
  LV   VG     Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  lab1 vg_lab -wi-ao---- <4.75g
----

[{format_cmd_exec}]
----
lvs -o lv_name,lv_size,lv_attr,segtype,devices vg_lab/lab1
----

[{format_cmd_output}]
----
  LV   LSize  Attr       Type   Devices
  lab1 <4.75g -wi-ao---- linear {disk1}(0)
----

[{format_cmd_exec}]
----
lvs --units g -o +devices vg_lab/lab1
----

[{format_cmd_output}]
----
  LV   VG     Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert Devices
  lab1 vg_lab -wi-ao---- 4.75g                                                     {disk1}(0)
----

[{format_cmd_exec}]
----
df /mnt/lab1
----

[{format_cmd_output}]
----
Filesystem              1K-blocks  Used Available Use% Mounted on
/dev/mapper/vg_lab-lab1   4832912 19448   4548248   1% /mnt/lab1
----

== Extend and Resize a Linear Volume

[{format_cmd_exec}]
----
pvcreate {disk2}
----

[{format_cmd_exec}]
----
vgextend vg_lab {disk2}
----

[{format_cmd_exec}]
----
lvresize -l 95%VG /dev/vg_lab/lab1
----

[{format_cmd_exec}]
----
resize2fs /dev/vg_lab/lab1
----

=== Examine Your Work

Let us take a look at the logical volume.  Notice a few things:

  * we added `seg_size` to the options to report segment size
  * the logical volume is comprised of 2 devices ({disk1},{disk2})
  * the first segment is completely used at 5g
  * the second segment is almost used, but has some space remaining
  * Over all, the volume group has approximately 500mb remaining

[{format_cmd_exec}]
----
lvs -o vg_name,vg_free,lv_name,lv_size,seg_size,segtype,devices vg_lab/lab1
----

[{format_cmd_output}]
----
  VG     VFree   LV   LSize  SSize  Type   Devices
  vg_lab 508.00m lab1 <9.50g <5.00g linear {disk1}(0)
  vg_lab 508.00m lab1 <9.50g  4.50g linear {disk2}(0)
----

[{format_cmd_exec}]
----
df /mnt/lab1
----

[{format_cmd_output}]
----
Filesystem              1K-blocks  Used Available Use% Mounted on
/dev/mapper/vg_lab-lab1   9735476 21840   9249360   1% /mnt/lab1
----

It is not always optimal to allocate 100% of volume group to the logical volumes.  For example, the unused space in the volume group could be used for a temporary snapshot.





== Create a RAID-10 Volume with Virtual Data Optimizer (VDO)

We will be leveraging devices {disk_glob}.  As before, we will cleanup up prior work and start fresh.

=== Clean Up Devices

Since we will be reusing the same resources for many exercises, we will begin by wiping everything clean.  Don't worry if you get an error message.

[{format_cmd_exec}]
----
umount /mnt/lab*
----

[{format_cmd_exec}]
----
vgremove -ff vg_lab
----

[{format_cmd_exec}]
----
pvremove {disk_glob}
----

[{format_cmd_exec}]
----
wipefs -a {disk_glob}
----

[{format_cmd_exec}]
----
partprobe
----

=== Physical Volume Creation

[{format_cmd_exec}]
----
pvcreate {disk_glob}
----

[{format_cmd_output}]
----
  Physical volume "{disk1}" successfully created.
  Physical volume "{disk2}" successfully created.
  Physical volume "{disk3}" successfully created.
  Physical volume "{disk4}" successfully created.
----



=== Volume Group Creation

[{format_cmd_exec}]
----
vgcreate vg_lab {disk_glob}
----

[{format_cmd_output}]
----
Volume group "vg_lab" successfully created
----



=== Logical Volume Creation

This time, we are going to use all four disks to create a mirrored set of striped disks.  Otherwise known as RAID10

[{format_cmd_exec}]
----
lvcreate -y --type raid10 -m1 -i 2 -n lv_raid10 -l 95%FREE vg_lab
----



=== Add VDO Deduplication


WARNING: (WORKSHOP VERSON 9.1) PLEASE NOTE THAT THE VDO PORTION OF THIS LAB IS NON-FUNCTIONAL 
AND TEMPORARILY DISABLED.  WE LOOK TOWARD THE 9.2 RELEASE TO REMEDIATE THIS ISSUE.


[{format_cmd_exec}]
----
lvconvert --type vdo-pool --name=lab2 -V 30G vg_lab/lv_raid10 -y
----

Once executed, you will be prompted to confirm the task.

[{format_cmd_output}]
----
 WARNING: Converting logical volume vg_lab/lv_raid10 to VDO pool volume with formating.
  THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
Do you really want to convert vg_lab/lv_raid10? [y/n]: y
    The VDO volume can address 6 GB in 3 data slabs, each 2 GB.
    It can grow to address at most 16 TB of physical storage in 8192 slabs.
    If a larger maximum size might be needed, use bigger slabs.
  device-mapper: remove ioctl on  (253:8) failed: Device or resource busy
  device-mapper: remove ioctl on  (253:8) failed: Device or resource busy
  device-mapper: remove ioctl on  (253:8) failed: Device or resource busy
  Logical volume "lab2" created.
  Converted vg_lab/lv_raid10 to VDO pool volume and created virtual vg_lab/lab2 VDO volume.
----

[{format_cmd_exec}]
----
mkfs.xfs -K /dev/mapper/vg_lab-lab2
----

[{format_cmd_exec}]
----
mkdir /mnt/lab2
----

[{format_cmd_exec}]
----
mount /dev/mapper/vg_lab-lab2 /mnt/lab2
----

NOTE: To make the mount persistent across reboots, you would still need to either add a systemd unit to mount the filesystem, or add an entry to /etc/fstab.

=== Create Sample Data

Let us now populate the filesystem with some content.  Create a bunch of random subdirectories in our new filesystems with the following command.

[{format_cmd_exec}]
----
for i in {1..100} ; do mktemp -d /mnt/lab2/XXXXXX ; done
----

Now we will copy the same content into each of the folders as follows.

NOTE: This could take a few minutes.

[{format_cmd_exec}]
----
for i in /mnt/lab2/* ; do echo "${i}" ; cp -rf /usr/share/locale $i ; done
----

The prevoius command should have copied approximately 100MB in 100 folders yielding about 10G of traditional fielsystem consumption.

=== Examine Your Work

Let us now check some statistics.  

[{format_cmd_exec}]
----
du -sh /mnt/lab2
----

[{format_cmd_exec}]
----
df /mnt/lab2
----

[{format_cmd_exec}]
----
vdostats --human-readable
----


[{format_cmd_output}]
----
Device                      Size      Used Available Use% Space saving%
vg_lab-lv_raid10-vpool      9.5G      3.7G      5.8G  39%           98%
----




So in summary, we built a 30GB filesystem that only has 10GB of actual physical disk capacity.  We then copied 10GB of data into the filesystem, but after deduplication `vdostats --human-readable` should reflect something near 5.5GB of available plysical space and a savings around 98%.

A few additional high-level things to know about VDO.  

VDO uses a high-performance deduplication index called UDS to detect duplicate blocks of data as they are being stored. The deduplication window is the number of previously written blocks which the index remembers. The size of the deduplication window is configurable.  The index will require a specific amount of RAM and a specific amount of disk space.

Red Hat generally recommends using a "sparse" UDS index for all production use cases. This indexing data structure requires approximately one-tenth of a byte of DRAM (memory) per block in its deduplication window. On disk, it requires approximately 72 bytes of disk space per block.

The default configuration of the index is to use a "dense" index. This index is considerably less efficient (by a factor of 10) in DRAM, but it has much lower (also by a factor of 10) minimum required disk space, making it more convenient for evaluation in constrained environments.

Please refer to the Red Hat Storage Administration Guide further information on provisioning and managing your data with VDO:

Red Hat Enterprise Linux Storage Administration Guide (VDO)



== Conclusion

This concludes the exercises related to lvm and vdo.

Time to finish this unit and return the shell to it's home position.

[{format_cmd_exec}]
----
workshop-finish-exercise.sh
----



== Additional Resources

    * link:https://www.redhat.com/en/blog/look-vdo-new-linux-compression-layer[A Look At VDO (BLOG)]

Red Hat Documentation

    * link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_storage_devices/index[Managing Storage Devices]
    * link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_logical_volumes/index[Managing Logical Volumes]
    * link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/deduplicating_and_compressing_logical_volumes_on_rhel/index[Deduplication and Compressing LVs]

[discrete]
== End of Unit

ifdef::env-github[]
link:../RHEL9-Workshop.adoc#toc[Return to TOC]
endif::[]

////
Always end files with a blank line to avoid include problems.
////

